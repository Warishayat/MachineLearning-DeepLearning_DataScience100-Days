# MachineLearning-DataScience100-Days
#100 days with Machine-Learning/Data-Science
<br>
Day1:
<br>
Today i read and practise all about csv files like how to import csv,and their functions like skiprows,indexcols,usecol, colums,na_pamater,convertor etc.
<br>
Day2:
<br>
Today i read and practise all about json and Sql files like how to import JSON,and How to work with Sql.
<br>
Day3:
<br>
Today i read and practise all about how to understand the data.
<br>
Day4:
<br>
Today i read about univariate Data Analysis and solve some practise level question.
<br>
Day5:
<br>
Today i read about bivariate Data Analysis and Multivariate Data analysis and solve some practise level datset.
<br>
Day6:
<br>
Today i read about Data_Profiling how to analyse the data in the form of html etc.
<br>
Day7:
<br>
Today i read about standard Scalar from Feature_Engineering.
<br>
Day8:
<br>
Today i read about normalization from Feature_Engineering.
<br>
Day9:
<br>
Today i read about Ordinanl Encoding from Feature_Engineering.
<br>
Day11:
<br>
Today i read about the ColumTransformer which is quite easy to do similar task that we do with labelencoder and ordinanl Encoder.
<br>
Day12:
<br>
Today i read about the sklearn pipelines and practise titanic datset.
<br>
Day13:
<br>
Today i read about the sklearn function transformer and practise titanic datset.
<br>
Day14:
<br>
Today i read about the sklearn power transformer.
<br>
Day15:
<br>
Today i make a project using all existing concept.
<br>
Day16:
<br>
Today i cover the mix data like how to handle mix data.
<br>
Day17:
<br>
Today i cover the how to handles date and time in the data.
<br>
Day18:
<br>
Today i cover the CCA -->Case complete analysis or removinf or droping the values.
<br>
Day19:
<br>
Today i cover the handling missing values or univariate handle mising values.
<br>
Day20:
<br>
Today i cover the handling missing categorical values.
<br>
Day21:
<br>
Today i cover the handling missing values with tecnique Fill with Random and Missing Indicator.
<br>
Day22:
<br>
Today i cover the Knn imputer and practise side by side comparision with simpleimputer,Knn imputer,missingindicator,random etc.
<br>
Day23:
<br>
Today i understand the concept of Multivariate Missing Imputer Concept.
<br>
Day24:
<br>
Today i wrote about the outlier at this day i take the concept of handling outlier with z score that mean data is normally distributed then we remove the outliers with tecniique  zcsore
<br>
like if i talk about the formoula that is: z_score=x-mean/std or 3+x(standard_diviation) for positive side and 3-x(standard_diviation) then we read the conceptt of capping and trrimming the data.
<br>
Day25:
<br>
Today i read about the another tecnique of handling outlier is IQR we do solve some question the formoula that we used in the IQR method is<br>
Q1-1.5*iqr and Q3+1.5*iqr .where IQR is the diffence between the value of 75% and 25%. and q1 is 25% of the total value where the q3 is the 75% of <br>
pf the total value.
<br>
Day25:
<br>
Today i read about the percentile tcehnique by using that how we can detact and remove outliers.
<br>
Day26:
<br>
Today i read about the feature contsruction like how to construct new columns from existing data and i learn how to do feature split..
<br>
Day27:
<br>
Today i read about the curse of dimensionality where the dimensionality mean feature and feature mean columns that if we have feature more then algoritham that can make the <br>
algoritham capacity decrease or it is not benifical for the algorithan  and it would increase the sparsity of the columns becuase of that the data point separate/fare from the mean<br>
in Curse of dimensionality use both for FEATURE SELECTION and FEATURE EXTRACTION.
<br>
Day28:
<br>
Today i read about the principle component analysis pca feature extraction technique what is that? how that is work.?
<br>
Day29:
<br>
Today i practise the old topic that i had done in past like future construction and handling mix data.
<br>
Day:30
<br>
Today i read about the simple linear regression how it is work but without mathematical intution.
<br>
Day:31
<br>
Today i read about the simple linear regression how it is work with mathamtics intutions.
<br>
Day:32
<br>
Today i read about the multiple linear regression how it is work without mathamtics intutions.
<br>
Day:33
<br>
Today i read about the multiple linear regression how it is work with mathamtics intutions.
<br>
Day:34
<br>
Today i read about the Mean Absoulute Error ,Mean Squared Error,Root mean Squared Error,R2_score and adjusted r2_score.
<br>
Day:35
<br>
Today i read about the Gradient descent
<br>
Day:36
<br>
Today i read about the Gradient descent type which is Batch Gradient Descent and their mathematical intuition and code from scracth.
<br>
Day:37
<br>
Today i read about the Gradient descent type which is stochastic Gradient Descent and their mathematical intuition. and code from scractch. And start practise and revise of all topic from start.
<br>
Day:38
<br>
Today i read about the Gradient descent type which is Minni Batch Gradient Descent and their mathematical intuition. and code from scractch.from start.
<br>
Day:39
<br>
First i revised the multiple linear regression and doin some paractise.
<br>
Than i read about the polynomial regression when the data is non linear.
<br>
Day:40
<br>
First i revised theGradient Descent and doin some paractise.
<br>
Then i read about Bias variance trade off mean the concept of underfitting and overfitting technique.
<br>
Then i read about about Regularization technique or ridge technique which i used for overfitting.
<br>
Day:41
<br>
i read about the ridge regression and solve problem on that.and make my own class from scratch.
<br>
i read about the ridge regression and solve problem with gradient decent by making my own class although there was some dimension error but i will check letar.
<br>
Day:42
<br>
i practise about the gradient descent type which is stochastic gradient descent.which take update with every row.
<br>
Day:43
<br>
i practise about the gradient descent type which is Minni Batch gradient descent.which take update with batch size.
<br>
i practise about the polynomial regression why that use what is the working behind that than practise a problem of polynomial.
<br>
Day:44
<br>
i practise about the Overfitting technique which is ridge regression i do that with sklearn library and with my own class and compare the accuracy between both of them.
<br>
Day:45
<br>
Today i read about the ridge regression key feature the five feature that are:
<br>
1:How to coefficent get affected by lambda?
<br>
2:Higher values are impace more?
<br>
3:Regularization effect on bais variance?
<br>
4:Lambda impact on loss function?
<br>
5:why it is called ridge?
<br>
Day:46
<br>
Today i read and practise about the lasso regression key features their background intutions:
<br>
Day:47
<br>
Today i read and practise about the lasso regression behind the mathematics like why lasso creat sparsity mean why coef_ gone zero if we increase lambda/alpha.
<br>
Than i read and practise about the Elasitic net regression which is another technique to reduce overfitting. it is the combination of both Ridge and Lasso.
<br>
Day:48
<br>
Today i read logistic regressiond and the basic concept of perceptron that using in logistic regression.
<br>
Day:49
<br>
Today i read about  logistic regressiond with sigmoid function.
<br>
Day:50
<br>
Today i read about the loss function of the logistic regression or function that sklearn used inside the logistic regression working.
<br>
Than i read about the classification metrics and i read about the accuracy score classifiaction and Confusion metrics.
<br>
Day:51
<br>
Today i read about the Precision metrics,Recall metrics and F1_score and doin some practise about the metrics.
<br>
i do some pracrise of the precession,recall,f1_score and classification report.
<br> 
Two days break because of some issue. 
