# MachineLearning-DataScience100-Days
#100 days with Machine-Learning/Data-Science
<br>
Day1:
<br>
Today i read and practise all about csv files like how to import csv,and their functions like skiprows,indexcols,usecol, colums,na_pamater,convertor etc.
<br>
Day2:
<br>
Today i read and practise all about json and Sql files like how to import JSON,and How to work with Sql.
<br>
Day3:
<br>
Today i read and practise all about how to understand the data.
<br>
Day4:
<br>
Today i read about univariate Data Analysis and solve some practise level question.
<br>
Day5:
<br>
Today i read about bivariate Data Analysis and Multivariate Data analysis and solve some practise level datset.
<br>
Day6:
<br>
Today i read about Data_Profiling how to analyse the data in the form of html etc.
<br>
Day7:
<br>
Today i read about standard Scalar from Feature_Engineering.
<br>
Day8:
<br>
Today i read about normalization from Feature_Engineering.
<br>
Day9:
<br>
Today i read about Ordinanl Encoding from Feature_Engineering.
<br>
Day11:
<br>
Today i read about the ColumTransformer which is quite easy to do similar task that we do with labelencoder and ordinanl Encoder.
<br>
Day12:
<br>
Today i read about the sklearn pipelines and practise titanic datset.
<br>
Day13:
<br>
Today i read about the sklearn function transformer and practise titanic datset.
<br>
Day14:
<br>
Today i read about the sklearn power transformer.
<br>
Day15:
<br>
Today i make a project using all existing concept.
<br>
Day16:
<br>
Today i cover the mix data like how to handle mix data.
<br>
Day17:
<br>
Today i cover the how to handles date and time in the data.
<br>
Day18:
<br>
Today i cover the CCA -->Case complete analysis or removinf or droping the values.
<br>
Day19:
<br>
Today i cover the handling missing values or univariate handle mising values.
<br>
Day20:
<br>
Today i cover the handling missing categorical values.
<br>
Day21:
<br>
Today i cover the handling missing values with tecnique Fill with Random and Missing Indicator.
<br>
Day22:
<br>
Today i cover the Knn imputer and practise side by side comparision with simpleimputer,Knn imputer,missingindicator,random etc.
<br>
Day23:
<br>
Today i understand the concept of Multivariate Missing Imputer Concept.
<br>
Day24:
<br>
Today i wrote about the outlier at this day i take the concept of handling outlier with z score that mean data is normally distributed then we remove the outliers with tecniique  zcsore
<br>
like if i talk about the formoula that is: z_score=x-mean/std or 3+x(standard_diviation) for positive side and 3-x(standard_diviation) then we read the conceptt of capping and trrimming the data.
<br>
Day25:
<br>
Today i read about the another tecnique of handling outlier is IQR we do solve some question the formoula that we used in the IQR method is<br>
Q1-1.5*iqr and Q3+1.5*iqr .where IQR is the diffence between the value of 75% and 25%. and q1 is 25% of the total value where the q3 is the 75% of <br>
pf the total value.
<br>
Day25:
<br>
Today i read about the percentile tcehnique by using that how we can detact and remove outliers.
<br>
Day26:
<br>
Today i read about the feature contsruction like how to construct new columns from existing data and i learn how to do feature split..
<br>
Day27:
<br>
Today i read about the curse of dimensionality where the dimensionality mean feature and feature mean columns that if we have feature more then algoritham that can make the <br>
algoritham capacity decrease or it is not benifical for the algorithan  and it would increase the sparsity of the columns becuase of that the data point separate/fare from the mean<br>
in Curse of dimensionality use both for FEATURE SELECTION and FEATURE EXTRACTION.
<br>
Day28:
<br>
Today i read about the principle component analysis pca feature extraction technique what is that? how that is work.?
<br>
Day29:
<br>
Today i practise the old topic that i had done in past like future construction and handling mix data.
<br>
Day:30
<br>
Today i read about the simple linear regression how it is work but without mathematical intution.
<br>
Day:31
<br>
Today i read about the simple linear regression how it is work with mathamtics intutions.
<br>
Day:32
<br>
Today i read about the multiple linear regression how it is work without mathamtics intutions.
<br>
Day:33
<br>
Today i read about the multiple linear regression how it is work with mathamtics intutions.
<br>
Day:34
<br>
Today i read about the Mean Absoulute Error ,Mean Squared Error,Root mean Squared Error,R2_score and adjusted r2_score.
<br>
Day:35
<br>
Today i read about the Gradient descent
<br>
Day:36
<br>
Today i read about the Gradient descent type which is Batch Gradient Descent and their mathematical intuition and code from scracth.
<br>
Day:37
<br>
Today i read about the Gradient descent type which is stochastic Gradient Descent and their mathematical intuition. and code from scractch. And start practise and revise of all topic from start.
<br>
Day:38
<br>
Today i read about the Gradient descent type which is Minni Batch Gradient Descent and their mathematical intuition. and code from scractch.from start.
<br>
Day:39
<br>
First i revised the multiple linear regression and doin some paractise.
<br>
Than i read about the polynomial regression when the data is non linear.
<br>
Day:40
<br>
First i revised theGradient Descent and doin some paractise.
<br>
Then i read about Bias variance trade off mean the concept of underfitting and overfitting technique.
<br>
Then i read about about Regularization technique or ridge technique which i used for overfitting.
<br>
Day:41
<br>
i read about the ridge regression and solve problem on that.and make my own class from scratch.
<br>
i read about the ridge regression and solve problem with gradient decent by making my own class although there was some dimension error but i will check letar.
<br>
Day:42
<br>
i practise about the gradient descent type which is stochastic gradient descent.which take update with every row.
<br>
Day:43
<br>
i practise about the gradient descent type which is Minni Batch gradient descent.which take update with batch size.
<br>
i practise about the polynomial regression why that use what is the working behind that than practise a problem of polynomial.
<br>
Day:44
<br>
i practise about the Overfitting technique which is ridge regression i do that with sklearn library and with my own class and compare the accuracy between both of them.
<br>
Day:45
<br>
Today i read about the ridge regression key feature the five feature that are:
<br>
1:How to coefficent get affected by lambda?
<br>
2:Higher values are impace more?
<br>
3:Regularization effect on bais variance?
<br>
4:Lambda impact on loss function?
<br>
5:why it is called ridge?
<br>
Day:46
<br>
Today i read and practise about the lasso regression key features their background intutions:
<br>
Day:47
<br>
Today i read and practise about the lasso regression behind the mathematics like why lasso creat sparsity mean why coef_ gone zero if we increase lambda/alpha.
<br>
Than i read and practise about the Elasitic net regression which is another technique to reduce overfitting. it is the combination of both Ridge and Lasso.
<br>
Day:48
<br>
Today i read logistic regressiond and the basic concept of perceptron that using in logistic regression.
<br>
Day:49
<br>
Today i read about  logistic regressiond with sigmoid function.
<br>
Day:50
<br>
Today i read about the loss function of the logistic regression or function that sklearn used inside the logistic regression working.
<br>
Than i read about the classification metrics and i read about the accuracy score classifiaction and Confusion metrics.
<br>
Day:51
<br>
Today i read about the Precision metrics,Recall metrics and F1_score and doin some practise about the metrics.
<br>
i do some pracrise of the precession,recall,f1_score and classification report.
<br> 
Day:52
<br>
Today i read about the softmax regression when we have more than two classes.
<br>
Than about polynomial logistic regression than i read about the hyperparameter of the logissti regression.
<br>
Day 53:
Today i read about the Disicion tree model what is disicion tree how it's work working of entropy etc. 
<br>
Today i read about the Disicion Tree hyper parameter.
<br>
Today i read about the Disicion regression tree and explore the librarry Dtreeviz.
<br>
Day 54:
<br>
Today i read about the ensemble technique, and about the type pf the technique.Like bagging voting etc.
<br>
Than i read about the voting ensamble assumption of voting ensamble.
<br>
Than i read about the classfication voting ensamble hard voting and soft voting and practise voting ensemble on the iris dataset.
<br>
I do the coding of voting ensemble using diffrrent base model like logistic regression svm and disicion tree. 
<br>
Than i read about the voting regressor.
<br>
Day 55:
<br>
Today i read about the bagging ensemble technique core idea of bagging intution of bagging etc. than i apply all the knowlede to a code.
<br>
Today i read about Bagging classifier and practise the code example of bagging ensemble with bootstraping than i read about the type of the bagging like pasting(without replacement) ,random subspaces (colum sampling) and random patches(both colums and row smapling).
<br>
Today i read about Bagging Regressor and doin a code example by using linear regression Disicion Tree regression and KnearestNeighbour with GridSearchCv to find the best aprameter.
<br>
Day 56:
Today i read about the random forest algoritham basic about the algoritham.
<br>
Day:57
<br>
Today i read about all about random forest what is difference between random forest and bagging ensemble,how to tune hyperparameter oob score each and everything all about random forest. 
<br>
Day:58
<br>
Today i read about the adaboost Classifier where i read about weak learner,disicion stump etc. 
<br>
Day:59
<br>
Today i read about the adaboost Classifier working from sctrach.
<br.
Than i implement the class through scikit learn and hpertune the parameter.than check which parametrs are the best for the adaboos using gridSeacrhCv.
<br>
Bagging vs boosting.what are the model that use, (LBHV, HBLV), parallel vs sequential model, and weights for the model. 
<br>
Day:60
<br>
Todai i read about K mean clustering back intuition how it's work how to know how much clusters should i take using Elbow method. 
<br>
Than i apply all the knowledge to a practical dataset. 
<br>
Than i make class from scratch of cluster and doin all the work manually. 
<br>
Than i make class from scratch of python.
<br>
Day:61
<br>
Today i explore the gradient boosting algorithm and the intuition of this algorithm. 
<br>
Today i read about the back math of gradient boosting.
<br>
Today i explore gradient boosting with classification.
<br>
Day:62
<br>
Today i explore stacking and blanding.
<br>
Than i do code of stacking and blending.
<br>
Day:63
<br>
Today i explore another clustering algo Agglomarative clustering which is unsupervised machine learning algorithm. 
<br>
The main difference behind KMean clusters and Agglomarative clusters is that the Kmean work well when the data /cluster is well defined on the other hand Agglomarative clustering work with complex data.
<br>
Day:64
<br>
Today i work with  k Nearest Neighbour their working code each and everything. 
<br>
Day:65
<br>
I explore the five assumption of Linear Regression. 
<br>
Day:66
<br>
Today i explore about the Support vector machine and their kernal trick each and every thing about svm.
<br>
Day:67
<br>
Today i explore he naive bayes algoritham internally which is used bayes algoritham.than i do a code without using any ml library.
<br>
Day:68
<br>
Today I learn about the XGBoost which is famous library made up on gradient boosting. I learn to code and example Xg-boost with regressor.
<br>
Than I explore the XGBoost classification. 
Ans solve the practical problem with a real world data set. 
<br>
day:69
<br>
Today i read about about the DBSCAN algoritham which is unsupervised learning algoritham technique.
<br>
Working on project Machine learning is complete now.
<br>
i upload a case study project which help the NGO to help the intenational countries that really need of funds.
<br>
Now i start Deep learning today i read about what is deep learning and machin learning diffrenc between deep and machine learning. Than what is deep learning complete introduction about deep learning.
<br>
Application of deep learning. 
<br>
Types of Perceptron. 
<br>
What is Perceptron? Singly layer Perceptron and Multi Layer Perceptron. 
<br>
Today i read about the loss function of Perceptron. 
<br>
Biggest flawn in Perceptron. 
<br>
Day:70 
<br>
Today i read about the Multi layer perceptron notations, and all about the multi layer perceptron.
<br>
Today i do another project on mnist dataset.
Today i read about forward propagation. 
<br>
Today i do a project by using Artificial Neural Network. 
<br>
Today i read about the different loss functions in deep learning like Mean square error, mean absolute error, huber loss, binary crossentropy, categorical_crossentropy, sparse_categorical_cross_entropy. 
<br>
Day:71
<br>
Today i read about what is back propagation how it's implementation, why its work and all the mathematics of back propagation.
<br>
Day:72 
<br>
Today i read baout the memoization technique where memoization mean to speed up the program.
<br>
Today i read about different varient of gradient descent like batch gradient descent, stochastic gradient descent and minni batch gradient descent. 
<br>
Today i read about vanishing gradient descent. 
<br>
Day:73 
<br>
Today i read about how to improve the perfoamance of the neural network, how to fie tune the hyperparameter,and how to tune the hyerparamter.
<br>
Than i read about the concept of early stopping mean how much i should iterate into the data.
<br>
Than i read another topic about the improving perfoamance of neura network that is feature scaling in artifiacal neral network.
<br>
Than i explore the Dropout layers which is another technique to reduce overfitting  in neural network which is used to improve the perfoamance of neural network.
<br>
Than i read about the another overfitting technique which is regularization. i explore L1 and L2 regularization in deep learning.
<br>
Than i explore the diffrent activation function like sigmoid,tanh hyperparabolic and Relu(Rectified linear unit).than explore the diffrent variant of relu like leaky relu, parametric relu,exponential relu and scaled relu.
<br>
Than i read about the Weight intialization and i explore the two weight initialization technique like Glorot_normal and Hi_normal.
<br>
Than i read about the batch normaization technique which is used to make faster training of neural network.
<br>
Than i read about diffrent optimizers like adagrad adam,nastrove accelerated gradient momentum etc.
<br>
Than i explore more about the optimizers like rmsprop,adam how all these diifrent from each others.
<br>
Today i read about hyperparameter tunner in Neural network.
<br>
Day:74
<br>
Today i read about the introduction oart of cnn. 
<br>
Today i read about the Convoulational Operations than Padding and strides than i read about the max pooling than i apply all the knowledge with a mnist dataset.
<br>
Today i read about back propagation in Cnn. 
<br>
Than i work on a project which was image classification Dog vs Cat. 
<br>
Day:75 
<br>
Today i read about the data augmentation technique to avoid from overfitting and when you have low data.
<br>
Than i explore the pretrained Model of keras like ResNeT ,GoggleNet,VGG16,etc
<br>
Today  i read about the transfar learnring when we use someones pretrained model with our custom data than we use transfar learning there are two type of learning 1: Feature Extraction and 2 is Fine tunning.
<br>
Today i read about functional model api. 
<br>
Day:76 
<br>
Today i read about RNN,applications of RNN than forward propagation in RNN.
<br>
Today i read about the Type of Rnn an i do a project with two diffrent technique like 1: is integer encode and the second one is Embedding.
<br>
Today i read about the back propagation in RNN & problem in RNN there are two major problem like Long term dependencies and unstable gradient. 
<br>
Day:77
<br>
Today i read about the lstm and the architecture of lstm.
<br>
forget gate,input gate and output gate. 